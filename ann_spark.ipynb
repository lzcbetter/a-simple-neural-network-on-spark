{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I learn best with toy code that I can play with. I believe that only practice can help you convert theory into your own knowledge.\n",
    "\n",
    "This tutorial teaches backpropagation via a very simple toy example, a short python implementation.\n",
    "\n",
    "This implementation is based on apache spark, train datasets are splited and stored in different nodes (via spark), network training (back propagation) is carried out via distributed evalution.\n",
    "\n",
    "results show that this spark based implementation is 1.85X faster (four cores were assigned for my virtual machine, 180 vs. 333 seconds) than my previous one in https://github.com/lzcbetter/step-by-step-neural-network, not 4X faster in theory because of a lot of reasons, like execution environment(python vs. py4j), communication, test dataset size, virtual machine, etc. Spark based implementaion can handle big dataset which is not affordable for a single node. More optimization need to do on this implementation\n",
    "\n",
    "tests were carried out on a virtual machine (provided in online course https://courses.edx.org/courses/BerkeleyX/CS190.1x/1T2015/info) on Macbook Pro, i7 CPU. four cores and 3GB RAM were assigned to the virtual machine\n",
    "\n",
    "Distributed process as followings:\n",
    "![alt text](flow_chart.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import necessary modules, numpy will be used for matrix and array with better performance\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "import math\n",
    "from operator import add\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_training_set():\n",
    "    X = np.load(\"X.npy\")\n",
    "    y = np.load(\"y.npy\")\n",
    "    dataset = []\n",
    "    for i in range(y.shape[0]):\n",
    "        x_vector = X[i].reshape((-1, 1))\n",
    "        y_vector = np.eye(10)[y[i]-1].reshape((-1, 1))\n",
    "        dataset.append((x_vector, y_vector))\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spark context (sc) was created in another place, which comes with the virtual machine os image\n",
    "def parse_data():\n",
    "    dataset = load_training_set()\n",
    "    data_set = sc.parallelize(dataset)\n",
    "    #split it into training, validation and test sets. Use the randomSplit method\n",
    "    weights = [.8, .1, .1]\n",
    "    seed = 8888\n",
    "    train_set, validate_set, test_set = data_set.randomSplit(weights, seed)\n",
    "    \n",
    "    return (train_set, validate_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# derivative of our sigmoid function, in terms of the output (i.e. y)\n",
    "def dsigmoid(y):\n",
    "    gz = sigmoid(y)\n",
    "    return gz * (1.0 - gz)\n",
    "\n",
    "def sigmoid(x):\n",
    "    # exp function provided by numpy can support vector operation by default\n",
    "    return 1.0 / (1.0 + np.exp(-x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, ni, nh, no):\n",
    "        # number of input, hidden, and output nodes\n",
    "        self.n1 = ni + 1 # +1 for bias node\n",
    "        self.n2 = nh\n",
    "        self.n3 = no\n",
    "        # create weights variables (the theta in model)\n",
    "        self.w1 = self.weights_init(ni, nh)\n",
    "        self.w2 = self.weights_init(nh, no)\n",
    "\n",
    "        # to accumulate the gradient from all the train samples   \n",
    "        self.Delta1 = np.zeros(shape=(self.n2, self.n1))  # for w1\n",
    "        self.Delta2 = np.zeros(shape=(self.n3, self.n2+1))# for w2\n",
    "        \n",
    "    def weights_init(self, l_in, l_out):\n",
    "            eps_init = 0.12\n",
    "            ret = np.random.rand(l_out, 1+l_in) * 2 * eps_init - eps_init\n",
    "            return ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ann_train_eval(w, sample):\n",
    "    w1, w2 = w\n",
    "    x = sample[0]\n",
    "    y = sample[1]\n",
    "    a1 = np.vstack(([1.0], x))\n",
    "\n",
    "    # hidden activations\n",
    "    z2 = np.dot(w1, a1)\n",
    "    a2 = np.vstack(([1.0], sigmoid(z2)))\n",
    "\n",
    "    # output activations\n",
    "    z3 = np.dot(w2, a2)\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    ## start back propogation\n",
    "    # calculate error terms for output\n",
    "    delta3 = a3 - y\n",
    "    # do not forget to skip the first column which is for bias and should not be included\n",
    "    delta2 = (np.dot(w2.T, delta3))[1:] * dsigmoid(z2)\n",
    "    \n",
    "    # gradient from all the train samples for accumulating\n",
    "    Delta1 = np.dot(delta2, a1.T)\n",
    "    Delta2 = np.dot(delta3, a2.T)\n",
    "\n",
    "    # mse for calculating train predict error, mse was used, just to show the minimization, \n",
    "    # if use other optimize method, should use cost function\n",
    "    \n",
    "    return np.array([Delta1, Delta2, np.mean((a3 - y)**2)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_predict(ann, inputs):\n",
    "    x = inputs.reshape((-1, 1))\n",
    "    a1 = np.vstack(([1.0], x))\n",
    "\n",
    "    # hidden activations\n",
    "    z2 = np.dot(ann.w1, a1)\n",
    "    a2 = np.vstack(([1.0], sigmoid(z2)))\n",
    "\n",
    "    # output activations\n",
    "    z3 = np.dot(ann.w2, a2)\n",
    "    a3 = sigmoid(z3)\n",
    "    return a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights update formula (gradient descent) $\\frac { \\partial  }{ \\partial { \\Theta  }_{ ij }^{ (l) } } J(\\Theta )={ D }_{ ij }^{ (l) }=\\frac { 1 }{ m } { \\Delta  }_{ ij }^{ (l) }+\\frac { \\lambda  }{ m } { \\Theta  }_{ ij }^{ (l) }$ \n",
    "\n",
    "where $\\lambda $ is the Regularization parameter, learn rate was set as 1.0\n",
    "\n",
    "paras:\n",
    "\n",
    "ann: the aritficial nural network to be trained\n",
    "\n",
    "train_set: dataset for train, one item in train_set should contain both x and y and must be ready to use in forward and back propogation.\n",
    "\n",
    "max_iter: number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_train(ann, train_set, max_iter):\n",
    "    Lambda = 10.01 # Regularization parameter (to avoid overfit issue)\n",
    "    m = train_set.count()\n",
    "    for iteration in range(max_iter):\n",
    "        eval_res = train_set.map(lambda x: ann_train_eval((ann.w1, ann.w2), x))\n",
    "        \n",
    "        # calculate derivation of weights via average bp results\n",
    "        average_eval = eval_res.reduce(add) / train_set.count()\n",
    "        dw1 = average_eval[0]\n",
    "        dw2 = average_eval[1]\n",
    "        \n",
    "        # mean error sololy for display\n",
    "        mean_err = average_eval[2]\n",
    "        \n",
    "        # for all the weights, you should not apply regulization on the first column since they are for bias\n",
    "        for i in range(ann.w1.shape[0]):\n",
    "            ann.w1[i, 0] = ann.w1[i, 0] - dw1[i, 0]\n",
    "        for i in range(ann.w1.shape[0]):\n",
    "            for j in range(1, ann.w1.shape[1]):\n",
    "                # here learn rate is 1.0, Lambda is the Regularization parameter\n",
    "                ann.w1[i, j] = ann.w1[i, j] - (dw1[i, j] + (Lambda/m) * ann.w1[i, j])\n",
    "        \n",
    "        for i in range(ann.w2.shape[0]):\n",
    "            ann.w2[i, 0] = ann.w2[i, 0] - dw2[i, 0]\n",
    "        for i in range(ann.w2.shape[0]):\n",
    "            for j in range(1, ann.w2.shape[1]):\n",
    "                ann.w2[i, j] = ann.w2[i, j] - (dw2[i, j] + (Lambda/m) * ann.w2[i, j])\n",
    "        if 0 == iteration % 50:\n",
    "            print (\"mean error p\", mean_err)\n",
    "    return ann\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3991 509 500\n",
      "train start at: 2015-09-06 15:50:37.267593\n",
      "('mean error p', 0.24472446057080668)\n",
      "('mean error p', 0.049078040391917648)\n",
      "('mean error p', 0.030102191020845506)\n",
      "('mean error p', 0.023516601844525017)\n",
      "('mean error p', 0.020482835158273722)\n",
      "('mean error p', 0.01881449971589234)\n",
      "('mean error p', 0.017766991561693434)\n",
      "('mean error p', 0.017048740290929683)\n",
      "train end at: 2015-09-06 15:54:14.445148\n",
      "time elapse in traininf: 217 seconds\n"
     ]
    }
   ],
   "source": [
    "# load and parse data, randomly devide dataset to three parts for train, validate and test separately \n",
    "train_set, validate_set, test_set = parse_data()\n",
    "#print train_set.count(), validate_set.count(), test_set.count()\n",
    "\n",
    "n = NN(400, 50, 10)\n",
    "\n",
    "# train it with some patterns\n",
    "dt_st = datetime.datetime.now()\n",
    "print (\"train start at: {0}\".format(dt_st))\n",
    "\n",
    "nt = nn_train(n, train_set, 400)\n",
    "\n",
    "dt_end = datetime.datetime.now()\n",
    "print (\"train end at: {0}\".format(dt_end))\n",
    "print (\"time elapse in traininf: {0} seconds\".format((dt_end-dt_st).seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set accuracy: 92.0070157855 %\n"
     ]
    }
   ],
   "source": [
    "# test trained neural network model on train dataset\n",
    "n_set = train_set.count()\n",
    "val_res = train_set.map(lambda x : 1 + np.argmax(nn_predict(nt, x[0]))).collect()\n",
    "actual_res = train_set.map(lambda x : 1 + np.argmax(x[1])).collect()\n",
    "\n",
    "accurate = 0\n",
    "for idx in range(n_set):\n",
    "    if val_res[idx] == actual_res[idx]:\n",
    "        accurate += 1\n",
    "print(\"train set accuracy: {0} %\".format(100.0 * accurate / n_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set accuracy: 89.5874263261 %\n"
     ]
    }
   ],
   "source": [
    "# test trained neural network model on validation data set\n",
    "n_val_set = validate_set.count()\n",
    "val_res = validate_set.map(lambda x : 1 + np.argmax(nn_predict(nt, x[0]))).collect()\n",
    "actual_res = validate_set.map(lambda x : 1 + np.argmax(x[1])).collect()\n",
    "\n",
    "accurate = 0\n",
    "for idx in range(n_val_set):\n",
    "    if val_res[idx] == actual_res[idx]:\n",
    "        accurate += 1\n",
    "print(\"validation set accuracy: {0} %\".format(100.0 * accurate / n_val_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy: 90.2 %\n"
     ]
    }
   ],
   "source": [
    "# test trained neural network model on validation data set\n",
    "n_val_set = test_set.count()\n",
    "\n",
    "val_res = test_set.map(lambda x : 1 + np.argmax(nn_predict(nt, x[0]))).collect()\n",
    "actual_res = test_set.map(lambda x : 1 + np.argmax(x[1])).collect()\n",
    "\n",
    "accurate = 0\n",
    "for idx in range(n_val_set):\n",
    "    if val_res[idx] == actual_res[idx]:\n",
    "        accurate += 1\n",
    "print(\"test set accuracy: {0} %\".format(100.0 * accurate / n_val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
