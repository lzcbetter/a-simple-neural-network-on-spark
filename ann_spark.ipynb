{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I learn best with toy code that I can play with. I believe that only practice can help you convert theory into your own knowledge.\n",
    "\n",
    "This tutorial teaches backpropagation via a very simple toy example, a short python implementation.\n",
    "\n",
    "This implementation is based on apache spark, train datasets are splited and stored in different nodes (via spark), network training (back propagation) is carried out via distributed evalution.\n",
    "\n",
    "results show that this spark based implementation is NOT faster (four cores were assigned for my virtual machine, ) than my previous one in https://github.com/lzcbetter/step-by-step-neural-network, this may because of a lot of reasons, like execution environment(python vs. py4j), communication, test dataset size, virtual machine, etc. Spark based implementaion can handle big dataset which is not affordable for a single node. More optimization need to do on this implementation\n",
    "\n",
    "tests were carried out on a virtual machine (provided in online course https://courses.edx.org/courses/BerkeleyX/CS190.1x/1T2015/info) on Macbook Pro, i7 CPU. four cores and 3GB RAM were assigned to the virtual machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import necessary modules, numpy will be used for matrix and array with better performance\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "import math\n",
    "from operator import add\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_training_set():\n",
    "    X = np.load(\"X.npy\")\n",
    "    y = np.load(\"y.npy\")\n",
    "    dataset = []\n",
    "    for i in range(y.shape[0]):\n",
    "        dataset.append((X[i], y[i]))\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spark context (sc) was created in another place, which comes with the virtual machine os image\n",
    "def parse_data():\n",
    "    dataset = load_training_set()\n",
    "    data_set = sc.parallelize(dataset)\n",
    "    \n",
    "    #split it into training, validation and test sets. Use the randomSplit method\n",
    "    weights = [.99, .005, .005]\n",
    "    seed = 42\n",
    "    train_set, validate_set, test_set = data_set.randomSplit(weights, seed)\n",
    "    \n",
    "    return (train_set, validate_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4949 23 28 0.0\n"
     ]
    }
   ],
   "source": [
    "train_set, validate_set, test_set = parse_data()\n",
    "print train_set.count(), validate_set.count(), test_set.count(), test_set.collect()[0][0][0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# derivative of our sigmoid function, in terms of the output (i.e. y)\n",
    "def dsigmoid(y):\n",
    "    gz = sigmoid(y)\n",
    "    return gz * (1.0 - gz)\n",
    "\n",
    "def sigmoid(x):\n",
    "    # exp function provided by numpy can support vector operation by default\n",
    "    return 1.0 / (1.0 + np.exp(-x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, ni, nh, no):\n",
    "        # number of input, hidden, and output nodes\n",
    "        self.n1 = ni + 1 # +1 for bias node\n",
    "        self.n2 = nh\n",
    "        self.n3 = no\n",
    "        # create weights variables (the theta in model)\n",
    "        self.w1 = self.weights_init(ni, nh)\n",
    "        self.w2 = self.weights_init(nh, no)\n",
    "\n",
    "        # to accumulate the gradient from all the train samples   \n",
    "        self.Delta1 = np.zeros(shape=(self.n2, self.n1))  # for w1\n",
    "        self.Delta2 = np.zeros(shape=(self.n3, self.n2+1))# for w2\n",
    "        \n",
    "    def weights_init(self, l_in, l_out):\n",
    "            eps_init = 0.12\n",
    "            ret = np.random.rand(l_out, 1+l_in) * 2 * eps_init - eps_init\n",
    "            return ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ann_train_eval(w, sample):\n",
    "    w1, w2 = w\n",
    "    x = sample[0].reshape((-1, 1))\n",
    "    y = np.eye(10)[sample[1]-1].reshape((-1, 1))\n",
    "    a1 = np.vstack(([1.0], x))\n",
    "\n",
    "    # hidden activations\n",
    "    z2 = np.dot(w1, a1)\n",
    "    a2 = np.vstack(([1.0], sigmoid(z2)))\n",
    "\n",
    "    # output activations\n",
    "    z3 = np.dot(w2, a2)\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    ## start back propogation\n",
    "    # calculate error terms for output\n",
    "    delta3 = a3 - y\n",
    "    # do not forget to skip the first column which is for bias and should not be included\n",
    "    delta2 = (np.dot(w2.T, delta3))[1:] * dsigmoid(z2)\n",
    "    \n",
    "    # gradient from all the train samples for accumulating\n",
    "    Delta1 = np.dot(delta2, a1.T)\n",
    "    Delta2 = np.dot(delta3, a2.T)\n",
    "\n",
    "    # calculate error, mse was used, just to show the minimization, \n",
    "    # if use other optimize method, should use cost function \n",
    "    \n",
    "    return np.array([Delta1, Delta2, abs(a3[sample[1]-1] - 1.0)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_predict(ann, inputs):\n",
    "    x = inputs.reshape((-1, 1))\n",
    "    a1 = np.vstack(([1.0], x))\n",
    "\n",
    "    # hidden activations\n",
    "    z2 = np.dot(ann.w1, a1)\n",
    "    a2 = np.vstack(([1.0], sigmoid(z2)))\n",
    "\n",
    "    # output activations\n",
    "    z3 = np.dot(ann.w2, a2)\n",
    "    a3 = sigmoid(z3)\n",
    "    return a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights update formula (gradient descent) $\\frac { \\partial  }{ \\partial { \\Theta  }_{ ij }^{ (l) } } J(\\Theta )={ D }_{ ij }^{ (l) }=\\frac { 1 }{ m } { \\Delta  }_{ ij }^{ (l) }+\\frac { \\lambda  }{ m } { \\Theta  }_{ ij }^{ (l) }$ \n",
    "\n",
    "where $\\lambda $ is the Regularization parameter, learn rate was set as 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_train(ann, train_set, max_iter):\n",
    "    Lambda = 10.01 # Regularization parameter (to avoid overfit issue)\n",
    "    m = train_set.count()\n",
    "    for iteration in range(max_iter):\n",
    "        eval_res = train_set.map(lambda x: ann_train_eval((ann.w1, ann.w2), x))\n",
    "        \n",
    "        # calculate derivation of weights via average bp results\n",
    "        #dw1 = eval_res.map(lambda x: x[0]).reduce(add) / train_set.count()\n",
    "        #dw2 = eval_res.map(lambda x: x[1]).reduce(add) / train_set.count()\n",
    "        \n",
    "        # mean error sololy for display\n",
    "        #mean_err = eval_res.map(lambda x: x[2]).reduce(add) / train_set.count()\n",
    "        #mean_err = 0.999\n",
    "        average_eval = eval_res.reduce(add) / train_set.count()\n",
    "        dw1 = average_eval[0]\n",
    "        dw2 = average_eval[1]\n",
    "        mean_err = average_eval[2]\n",
    "        \n",
    "        # for all the weights, you should not apply regulization on the first column since they are for bias\n",
    "        for i in range(ann.w1.shape[0]):\n",
    "            ann.w1[i, 0] = ann.w1[i, 0] - dw1[i, 0]\n",
    "        for i in range(ann.w1.shape[0]):\n",
    "            for j in range(1, ann.w1.shape[1]):\n",
    "                # here learn rate is 1.0, Lambda is the Regularization parameter\n",
    "                ann.w1[i, j] = ann.w1[i, j] - (dw1[i, j] + (Lambda/m) * ann.w1[i, j])\n",
    "        \n",
    "        for i in range(ann.w2.shape[0]):\n",
    "            ann.w2[i, 0] = ann.w2[i, 0] - dw2[i, 0]\n",
    "        for i in range(ann.w2.shape[0]):\n",
    "            for j in range(1, ann.w2.shape[1]):\n",
    "                ann.w2[i, j] = ann.w2[i, j] - (dw2[i, j] + (Lambda/m) * ann.w2[i, j])\n",
    "        if 0 == iteration % 50:\n",
    "            print (\"mean error p\", mean_err)\n",
    "    return ann\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start at: 2015-09-04 12:53:43.018569\n",
      "('mean error p', array([[ 0.4948555]]))\n",
      "('mean error p', array([[ 0.65209914]]))\n",
      "('mean error p', array([[ 0.45015289]]))\n",
      "('mean error p', array([[ 0.36078226]]))\n",
      "('mean error p', array([[ 0.31172843]]))\n",
      "('mean error p', array([[ 0.28234969]]))\n",
      "('mean error p', array([[ 0.26387839]]))\n",
      "('mean error p', array([[ 0.25165851]]))\n",
      "train end at: 2015-09-04 12:56:43.810599\n",
      "time elapse in traininf: 180 seconds\n"
     ]
    }
   ],
   "source": [
    "n = NN(400, 25, 10)\n",
    "\n",
    "# train it with some patterns\n",
    "dt_st = datetime.datetime.now()\n",
    "print (\"train start at: {0}\".format(dt_st))\n",
    "\n",
    "nt = nn_train(n, train_set, 400)\n",
    "\n",
    "dt_end = datetime.datetime.now()\n",
    "print (\"train end at: {0}\".format(dt_end))\n",
    "print (\"time elapse in traininf: {0} seconds\".format((dt_end-dt_st).seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_val_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b8b3b550a692>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0maccurate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_val_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mval_res\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mactual_res\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0maccurate\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_val_set' is not defined"
     ]
    }
   ],
   "source": [
    "# test trained neural network on train dataset\n",
    "n_set = train_set.count()\n",
    "\n",
    "val_res = train_set.map(lambda x : 1 + np.argmax(nn_predict(nt, x[0]))).collect()\n",
    "actual_res = train_set.collect()\n",
    "\n",
    "accurate = 0\n",
    "for idx in range(n_set):\n",
    "    if val_res[idx] == actual_res[idx][1]:\n",
    "        accurate += 1\n",
    "print(\"train set accuracy: {0} %\".format(100.0 * accurate / n_val_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test trained neural network on validation data set\n",
    "n_val_set = validate_set.count()\n",
    "\n",
    "val_res = validate_set.map(lambda x : 1 + np.argmax(nn_predict(nt, x[0]))).collect()\n",
    "actual_res = validate_set.collect()\n",
    "\n",
    "accurate = 0\n",
    "for idx in range(n_val_set):\n",
    "    if val_res[idx] == actual_res[idx][1]:\n",
    "        accurate += 1\n",
    "print(\"validation set accuracy: {0} %\".format(100.0 * accurate / n_val_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test trained neural network on validation data set\n",
    "n_val_set = test_set.count()\n",
    "\n",
    "val_res = test_set.map(lambda x : 1 + np.argmax(nn_predict(nt, x[0]))).collect()\n",
    "actual_res = test_set.collect()\n",
    "\n",
    "accurate = 0\n",
    "for idx in range(n_val_set):\n",
    "    if val_res[idx] == actual_res[idx][1]:\n",
    "        accurate += 1\n",
    "print(\"test set accuracy: {0} %\".format(100.0 * accurate / n_val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
